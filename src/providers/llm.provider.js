const { askClaudeSonnet, ClaudeModel } = require("./anthropic.provider");
const { prompt } = require("inquirer");
const { setEnvVar, getEnvVar } = require("../utils/env.util");
const { askChatGpt4oMini, askOpenAiO1Mini, Gpt4oMiniModel, O1MiniModel } = require("./openai.provider");

const FullModelList = [
    ClaudeModel,
    // O1MiniModel,
    Gpt4oMiniModel
];

/**
 * @function tryAskLLM
 * @param {string} fileType The type of React file being generated (fc|sw|hook|sw)
 * @param {string} fileName The name of the file or component
 * @description Tries to ask AI with a prompt to generate code
 * @returns {string} The code generated by AI or undefined if the user chose not to use AI
 */
async function tryAskLLM(fileType = 'fc', fileName = '') {
    // Prompt user if they want to give a description to claude
    let codeResult;

    let dftModelId = await getEnvVar('DEFAULT_MODEL_ID');

    console.log('\n')
    const { codePrompt } = await prompt([
        {
            type: 'input',
            name: 'codePrompt',
            message: `Prompt for generating code w/AI?\n (ignore and press ENTER to not use AI)\n`,
        },
    ]);
    if (codePrompt) {
        const _modelPrefFrequency = await getEnvVar('AI_MODEL_PREF_CHG_FREQ');
        if (!dftModelId || !_modelPrefFrequency || _modelPrefFrequency === 'EACH_TIME') {
            console.log('\n')
            const { aiProviderPref } = await prompt([
                {
                    type: 'list',
                    name: 'aiProviderPref',
                    message: `What AI model would you like to use?\n`,
                    default: dftModelId || ClaudeModel.value,
                    choices: FullModelList,
                }
            ]);

            if (!dftModelId) {
                const { modelPrefFrequency } = await prompt([
                    {
                        type: 'list',
                        name: 'modelPrefFrequency',
                        message: `How often would you like select a specific model?\n`,
                        default: 'ALWAYS_ASK',
                        choices: [
                            {
                                name: 'Ask for each command',
                                short: 'Ask each time',
                                value: 'ALWAYS_ASK',
                            },
                            {
                                name: 'Always use this model',
                                short: 'Always this model',
                                value: 'NEVER_ASK',
                            },
                        ],
                    },
                ]);
                await setEnvVar('AI_MODEL_PREF_CHG_FREQ', modelPrefFrequency);
            }

            dftModelId = aiProviderPref;
            await setEnvVar('DEFAULT_MODEL_ID', dftModelId, true);
        }
        
        const dftModel = FullModelList.find(m => m.value == dftModelId);
        if (!dftModel) {
            throw new Error(`Model with ID '${dftModelId || ''}' not found`);
        }
        try {
            switch (dftModel.value) {
                case Gpt4oMiniModel.value:
                    codeResult = await askChatGpt4oMini(codePrompt, fileType, fileName);
                    break;
                case O1MiniModel.value:
                    codeResult = await askOpenAiO1Mini(codePrompt, fileType, fileName);
                    break;
                case ClaudeModel.value:
                default:
                    codeResult = await askClaudeSonnet(codePrompt, fileType, fileName);
                    break;
            }
        } catch (error) {
            console.log('\n\n');
            throw new Error(error.message);
        }

        return codeResult;
    } else if (!codePrompt){
        console.log('\n');
        return;
    }
}



module.exports = { tryAskLLM };